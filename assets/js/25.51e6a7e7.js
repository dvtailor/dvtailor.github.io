(window.webpackJsonp=window.webpackJsonp||[]).push([[25],{320:function(e,t,r){},340:function(e,t,r){"use strict";r(320)},359:function(e,t,r){"use strict";r.r(t);r(340);var a=r(25),n=Object(a.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h2",{attrs:{id:"publications"}},[e._v("Publications")]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/approx-fullcp.png"}},[t("p",[t("strong",[t("p",{staticStyle:{"font-size":"16px"}},[e._v("Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence")])]),e._v(" "),t("strong",[t("u",[e._v("Dharmesh Tailor")])]),e._v(", Alvaro Correia, Eric Nalisnick, Christos Louizos")]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("13th International Conference on Learning Representations (ICLR)")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2025")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://openreview.net/forum?id=vcX0k4rGTt",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1),e._v(" /")]),e._v(" "),t("p",{staticStyle:{"font-size":"14px","padding-top":"0.5rem"}},[e._v("In this work, we construct prediction intervals for neural network regressors post-hoc without held-out data. This is achieved by approximating the full conformal prediction method (full-CP). Whilst full-CP nominally requires retraining the model for every test point and candidate label, we propose to train just once and locally perturb model parameters using Gauss-Newton influence to approximate the effect of retraining. On standard regression benchmarks and bounding box localization, we show the resulting prediction intervals are locally-adaptive and often tighter than those of split-CP.")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/l2d_meta.png"}},[t("p",[t("strong",[t("p",{staticStyle:{"font-size":"16px"}},[e._v("Learning to Defer to a Population: A Meta-Learning Approach")])]),e._v(" "),t("strong",[t("u",[e._v("Dharmesh Tailor")])]),e._v(", Aditya Patra, Rajeev Verma, Putra Manggala, Eric Nalisnick")]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("27th International Conference on Artificial Intelligence and Statistics (AISTATS)")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2024")])]),e._v(" "),t("p",{staticStyle:{color:"red","font-size":"14px"}},[e._v("Oral presentation & Outstanding Student Paper award (top-1% of accepted papers)")]),e._v(" "),t("p",[t("a",{attrs:{href:"https://proceedings.mlr.press/v238/tailor24a.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://arxiv.org/abs/2403.02683",target:"_blank",rel:"noopener noreferrer"}},[e._v("arXiv"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://github.com/dvtailor/meta-l2d",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"../docs/poster_aistats24.pdf"}},[e._v("poster")]),e._v(" / "),t("a",{attrs:{href:"../docs/slides_aistats24.pdf"}},[e._v("slides")])]),e._v(" "),t("p",{staticStyle:{"font-size":"14px","padding-top":"0.5rem"}},[e._v("We formulate a learning to defer (L2D) system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert’s abilities.")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/memory-perturbation.png"}},[t("p",[t("strong",[t("p",{staticStyle:{"font-size":"16px"}},[e._v("The Memory-Perturbation Equation: Understanding Model's Sensitivity to Data")])]),e._v("\nPeter Nickl, Lu Xu*, "),t("u",[t("strong",[e._v("Dharmesh Tailor")])]),e._v("*, Thomas Möllenhoff, Emtiyaz Khan")]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("37th Conference on Neural Information Processing Systems (NeurIPS)")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2023")])]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("ICML 2023 Workshop on Principles of Duality for Modern Machine Learning")])])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://papers.nips.cc/paper_files/paper/2023/hash/550ab405d0addd3de5b70e57b44878df-Abstract-Conference.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://arxiv.org/abs/2310.19273",target:"_blank",rel:"noopener noreferrer"}},[e._v("arXiv"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://github.com/team-approx-bayes/memory-perturbation",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://pnickl.github.io/docs/mpe_neurips23.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("poster"),t("OutboundLink")],1)]),e._v(" "),t("p",{staticStyle:{"font-size":"14px","padding-top":"0.5rem"}},[e._v("We present the Memory-Perturbation Equation (MPE) which relates model’s sensitivity to perturbation in its training data. Derived using Bayesian principles, the MPE unifies existing sensitivity measures, generalizes them to a wide-variety of models and algorithms, and unravels useful properties regarding sensitivities. Our empirical results show that sensitivity estimates obtained during training can be used to faithfully predict generalization on unseen test data.")])]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/neural-process.png"}},[t("p",[t("strong",[t("p",{staticStyle:{"font-size":"16px"}},[e._v("Exploiting Inferential Structure in Neural Processes")])]),e._v(" "),t("u",[t("strong",[e._v("Dharmesh Tailor")])]),e._v(", Emtiyaz Khan, Eric Nalisnick")]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("39th Conference on Uncertainty in Artificial Intelligence (UAI)")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2023")])]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("5th Workshop on Tractable Probabilistic Modeling at UAI 2022")])])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://proceedings.mlr.press/v216/tailor23a.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://arxiv.org/abs/2306.15169",target:"_blank",rel:"noopener noreferrer"}},[e._v("arXiv"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"../docs/poster_uai23.pdf"}},[e._v("poster")])]),e._v(" "),t("p",{staticStyle:{"font-size":"14px","padding-top":"0.5rem"}},[e._v("This work provides a framework that allows the latent variable of Neural Processes to be given a rich prior defined by a graphical model. These distributional assumptions directly translate into an appropriate aggregation strategy for the context set. We describe a message-passing procedure that still allows for end-to-end optimization with stochastic gradients. We demonstrate the generality of our framework by using mixture and Student-t assumptions that yield improvements in function modelling and test-time robustness.")])]),e._v(" "),t("p",[e._v(" ")]),e._v(" "),t("h3",{attrs:{id:"publications-pre-phd"}},[e._v("Publications (Pre-PhD)")]),e._v(" "),t("p",[e._v("These are papers published pre-PhD at the European Space Agency and University of Edinburgh.")]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/stability.png",hideBorder:"true"}},[t("p",[e._v(" ")]),e._v(" "),t("p",[t("strong",[t("p",{staticStyle:{"font-size":"15px"}},[e._v("On the Stability Analysis of Deep Neural Network Representations of an Optimal State-Feedback")])]),e._v("\nDario Izzo, "),t("u",[t("strong",[e._v("Dharmesh Tailor")])]),e._v(", Thomas Vasileiou")]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("IEEE Transactions on Aerospace and Electronic Systems")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2020")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://ieeexplore.ieee.org/document/9149837",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://arxiv.org/abs/1812.02532",target:"_blank",rel:"noopener noreferrer"}},[e._v("arXiv"),t("OutboundLink")],1)])]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/imitation.png",hideBorder:"true"}},[t("p",[e._v(" ")]),e._v(" "),t("p",[t("strong",[t("p",{staticStyle:{"font-size":"15px"}},[e._v("Learning the Optimal State-Feedback via Supervised Imitation Learning")])]),e._v(" "),t("u",[t("strong",[e._v("Dharmesh Tailor")])]),e._v(", Dario Izzo")]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("Astrodynamics (Springer)")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2019")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://link.springer.com/article/10.1007/s42064-019-0054-0",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://arxiv.org/abs/1901.02369",target:"_blank",rel:"noopener noreferrer"}},[e._v("arXiv"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://github.com/dvtailor/dnn-control",target:"_blank",rel:"noopener noreferrer"}},[e._v("code"),t("OutboundLink")],1)])]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/interplanetary.png",hideBorder:"true"}},[t("p",[e._v(" ")]),e._v(" "),t("p",[t("strong",[t("p",{staticStyle:{"font-size":"15px"}},[e._v("Machine Learning and Evolutionary Techniques in Interplanetary Trajectory Design")])]),e._v("\nDario Izzo, Christopher Sprague, "),t("u",[t("strong",[e._v("Dharmesh Tailor")])])]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("Modeling and Optimization in Space Engineering (Springer)")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2019")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://link.springer.com/chapter/10.1007/978-3-030-10501-3_8",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1),e._v(" / "),t("a",{attrs:{href:"https://arxiv.org/abs/1802.00180",target:"_blank",rel:"noopener noreferrer"}},[e._v("arXiv"),t("OutboundLink")],1)])]),e._v(" "),t("ProjectCard",{attrs:{image:"/images/neural-populations.png",hideBorder:"true"}},[t("p",[e._v(" ")]),e._v(" "),t("p",[t("strong",[t("p",{staticStyle:{"font-size":"15px"}},[e._v("Unconscious Biases in Neural Populations Coding Multiple Stimuli")])]),e._v("\nSander Keemink, "),t("u",[t("strong",[e._v("Dharmesh Tailor")])]),e._v(", Mark van Rossum")]),e._v(" "),t("p",[t("em",[t("span",{staticStyle:{"font-size":"14px"}},[e._v("Neural Computation")])]),e._v(", "),t("span",{staticStyle:{"font-size":"14px"}},[e._v("2018")])]),e._v(" "),t("p",[t("a",{attrs:{href:"https://www.mitpressjournals.org/doi/full/10.1162/neco_a_01130",target:"_blank",rel:"noopener noreferrer"}},[e._v("paper"),t("OutboundLink")],1)])])],1)}),[],!1,null,null,null);t.default=n.exports}}]);